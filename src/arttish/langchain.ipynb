{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in c:\\практика\\etu-smartchat\\.venv\\lib\\site-packages (0.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\практика\\etu-smartchat\\.venv\\lib\\site-packages (from llama-cpp-python) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\практика\\etu-smartchat\\.venv\\lib\\site-packages (from llama-cpp-python) (2.2.6)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\практика\\etu-smartchat\\.venv\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\практика\\etu-smartchat\\.venv\\lib\\site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\практика\\etu-smartchat\\.venv\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\практика\\etu-smartchat\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\практика\\etu-smartchat\\.venv\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:53:20.411700Z",
     "start_time": "2025-05-27T16:53:18.061285Z"
    }
   },
   "id": "1d322a4e65c88f7c",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Qwen2.5-7B-Instruct-Q4_K_M.gguf is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "model_repo = \"gaianet/Qwen2.5-7B-Instruct-GGUF\"\n",
    "model_basename = \"Qwen2.5-7B-Instruct-Q4_K_M.gguf\"\n",
    "\n",
    "model_path = \"../models/\" + model_basename\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=model_repo,\n",
    "        filename=model_basename,\n",
    "        local_dir=\"./models\",\n",
    "    )\n",
    "else:\n",
    "    print(f\"Model {model_basename} is already downloaded.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:53:20.657531Z",
     "start_time": "2025-05-27T16:53:20.412699Z"
    }
   },
   "id": "e9b7d60c92f79fa0",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "\n",
    "llm = ChatLlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024,\n",
    "    n_ctx=32768,\n",
    "    n_gpu_layers=40,\n",
    "    n_batch=512,\n",
    "    verbose=False,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:53:32.191128Z",
     "start_time": "2025-05-27T16:53:20.658530Z"
    }
   },
   "id": "3fc1e4321e3c0db",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "test_response = llm.invoke(\"Say 'Hello!'\")\n",
    "print(test_response.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:53:36.072432Z",
     "start_time": "2025-05-27T16:53:32.225129Z"
    }
   },
   "id": "76c338ad7a9e8fd0",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables import Runnable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:53:36.274051Z",
     "start_time": "2025-05-27T16:53:36.074431Z"
    }
   },
   "id": "e6f8d9bbcce9d236",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "template_string = \"\"\"<s>[INST] Ты являешься помошником. Отвечай на запросы пользователя используя *только* представленный ниже контекст. Не используй никакой иной информации. Если ответа в контесте нет, то говори, что ты не знаешь\n",
    "\n",
    "Контекст: {context}\n",
    "\n",
    "Запрос: {user_query} [/INST]\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:53:36.289325Z",
     "start_time": "2025-05-27T16:53:36.275052Z"
    }
   },
   "id": "6af550c04c1d5bbf",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "user_query = \"Что такое RAG?\"\n",
    "\n",
    "context = \"Генерация, дополненная поиском (Retrieval-Augmented Generation, RAG) — это подход, при котором генерация ответа большой языковой модели (LLM) осуществляется на основе данных, полученных в результате поиска во внешних источниках (файлы, базы данных, Интернет и другие источники). RAG-система работает в два основных этапа: сначала происходит извлечение релевантных документов или их частей из внешней базы знаний на основе запроса пользователя, а затем полученная информация подставляется вместе со специальными подсказками, указывающими как модель должна использовать эти данные, в контекст языковой модели для генерации итогового ответа. В зависимости от указаний в подсказках, сгенерированный ответ может включать цитаты или ссылки на исходные документы, что повышает прозрачность и доверие пользователей, позволяя проверить информацию. RAG помогает устранить ограничения LLM, такие как устаревание информации, наличие неточностей или появления галлюцинаций[1][2].\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:53:36.304906Z",
     "start_time": "2025-05-27T16:53:36.291325Z"
    }
   },
   "id": "f10cdfa1b10451d5",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG — это сокращение от \"Retrieval-Augmented Generation\". Этот подход использует генерацию ответа большой языковой модели (LLM), но добавляет к этому процессу данные, полученные в результате поиска во внешних источниках.\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt_message = prompt_template.format_messages(user_query=user_query, context=context)\n",
    "\n",
    "print(llm.invoke(formatted_prompt_message).content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:53:54.386510Z",
     "start_time": "2025-05-27T16:53:36.306461Z"
    }
   },
   "id": "74067b8194ff2185",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T16:53:54.402595Z",
     "start_time": "2025-05-27T16:53:54.387509Z"
    }
   },
   "id": "dd2ef8d400e4fbc8",
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
